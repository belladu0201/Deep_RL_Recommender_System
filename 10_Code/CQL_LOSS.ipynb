{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CQL Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Q(s,a) \\leq r(s,a) + \\gamma \\cdot \\max_{a'}(Q(s',a')) - \\alpha $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CQL algorithm\n",
    "class CQL:\n",
    "    def __init__(self, q_function, policy_network, discount_factor=0.6, alpha=0.01):\n",
    "        self.q_function = q_function\n",
    "        self.policy_network = policy_network\n",
    "        self.discount_factor = discount_factor\n",
    "        self.alpha = alpha\n",
    "        \n",
    "def update(self, states, actions, rewards, next_states, dones):\n",
    "        # Compute the Q-function targets\n",
    "        q_targets = rewards + np.max(self.q_function.predict(next_states), axis=1) * (1 - dones) * self.discount_factor\n",
    "        \n",
    "        # Compute the Q-values for the current states and actions\n",
    "        q_values = self.q_function.predict(states)\n",
    "        q_values = np.sum(q_values * actions, axis=1)\n",
    "        \n",
    "        # Compute the Q-value constraint\n",
    "        policy_probs = self.policy_network.predict(states)\n",
    "        q_constraint = q_values - self.alpha * np.log(policy_probs + 1e-8)\n",
    "        \n",
    "        # Compute the Q-function loss\n",
    "        q_loss = tf.keras.losses.MSE(q_values, q_targets)\n",
    "        \n",
    "        # Compute the policy loss\n",
    "        entropy = -tf.reduce_mean(policy_probs * tf.math.log(policy_probs + 1e-8))\n",
    "        policy_loss = -tf.reduce_mean(q_constraint) + 0.1 * entropy\n",
    "        \n",
    "        # Compute the total loss\n",
    "        loss = q_loss + policy_loss\n",
    "        \n",
    "        # Update the Q-function\n",
    "        self.q_function.train_on_batch(states, q_targets)\n",
    "        \n",
    "        # Update the policy network\n",
    "        self.policy_network.train_on_batch(states, actions)\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_q_function(state_dim, action_dim, num_hidden_layers=2, hidden_size=128):\n",
    "    inputs = tf.keras.layers.Input(shape=(state_dim,))\n",
    "    x = inputs\n",
    "    for _ in range(num_hidden_layers):\n",
    "        x = tf.keras.layers.Dense(hidden_size, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(action_dim)(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "def build_policy_network(state_dim, action_dim, num_hidden_layers=2, hidden_size=128):\n",
    "    inputs = tf.keras.layers.Input(shape=(state_dim,))\n",
    "    x = inputs\n",
    "    for _ in range(num_hidden_layers):\n",
    "        x = tf.keras.layers.Dense(hidden_size, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(action_dim, activation='softmax')(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "[1] https://arxiv.org/abs/2006.04779\n",
    "\n",
    "[2] Online Tools"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
